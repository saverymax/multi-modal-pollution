---
title: "VAR and Transformer Forecast evaluation"
author: "Max Savery"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{caption}
   - \usepackage{subcaption}

output:
  pdf_document:
   keep_tex: TRUE
  html_document: default
---
```{r setup, include=FALSE, results='hide', echo=FALSE}
knitr::is_latex_output()
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stringr)
library(kableExtra)
#  Use the development version from github, so we can use the 
# positive-ensured variance estimator
# install.packages("remotes")
# remotes::install_github("robjhyndman/forecast")
library(forecast)
```
```{r env, results='hide', echo=FALSE}
# Note that the figures generated by wandb are stored in the evaluation directory
# so that they can be generated in the pdf created by this markdown script.
user_dir = "D:/asus_documents/ku_leuven/thesis/code/multi-modal-pollution/src/evaluation"
setwd(user_dir)
```
\section{Model comparison}

We have so far discussed the data used for this work, given a brief theoretical discussion of the models used for forecasting, and described the means by which we can compare their performance. Therefore, we are now prepared to examine the ability of the transformer and the VAR model to forecast air pollution during the lockdowns in Brussels during the COVID-19 pandemic. We compare the models using forecasts generated from the test set, which contains time series of air pollution for the the period of April 24 to June 1st, 2021. The reason for this particular split was mentioned previously. We compare forecasts from the models for each measuring station, for NO$_2$, PM$_{2.5}$, and PM$_{10}$. The RMSE and MAE metrics are shown in Tables \ref{tab:lr-1e-4_pm25_h-1}, \ref{tab:lr-1e-4_pm10_h-1}, and \ref{tab:lr-1e-4_no2_h-1}. These tables show the forecasts from a transformer trained with a learning rate of 1e-4 only, and only 1 step ahead forecasts. Appendix \ref{appendix:model_comp} shows these results for additional horizons of 5 and 10. Furthermore, Appendix \ref{appendix:lre5} shows these results also using a transformer trained with a learning rate of 1e-5, as well as the DM and ADF tests for the learning rate of 1e-5 experiments. This motivates our next experiment in which we compare the effect of the learning rate on the transformer forecasts. This will be discussed in section \ref{sec:lr_comparison}.

The model performance differs somewhat depending on the pollutant in question. However, across all pollutants for a horizon of 1, we can see that the VAR model has a lower RMSE and MAE. Table \ref{tab:mean_df_lr_1e-4} shows the mean difference between the models for each pollutant metric. where the VAR model outperforms the transformer by approximately 1.5-2 units of RMSE and 1-1.5 units of MAE on average.

To compare the extent to which the forecasts are different, we compute the DM statistic between each series of forecast errors, for each station and pollutant. That is, for the respective series of observed and predicted values using horizon $h$ for a given air measuring station, $y_{1+h}, y_{2+h},...,y_{K}$ and $\hat{y}_{1+h},\hat{y}_{1+h},...,\hat{y}_{K}$, we then compute the DM test on the difference of these two series. These are two-sided tests, so that a significant result indicates that the two forecasts are significantly different than each other. Note that we show only results for comparing difference of the absolute errors (as opposed to the squared errors). The p-values from the DM tests are shown in Tables \ref{tab:dm_tests_lr-1e-4_no2}, \ref{tab:dm_tests_lr-1e-4_pm25}, and \ref{tab:dm_tests_lr-1e-4_pm10}, where $p\le.05$ is shown in bold. As mentioned in section \ref{sec:dm_test}, the DM test holds if the difference between there errors is stationary. Appendix \ref{appendix:model_comp} also shows the p-values generated by the Augmented Dickey Fuller test, which tests if the differences of the errors are in fact stationary. Note that in the interpretation of this test, the null hypothesis tests for non-stationarity. Therefore, rejecting this test is evidence in support of stationarity. The majority of the error differences result in the rejection of the test, so that the hypothesis of stationarity is supported and the assumption of the DM test is satisfied. 
```{r learning-rate-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
# echo=FALSE removes code from pdf; results='hide' removes code output
# Useful resources:
# https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# https://stackoverflow.com/questions/53153537/rmarkdown-setting-the-position-of-kable
# https://bookdown.org/yihui/rmarkdown-cookbook/hide-one.html
source("evaluation_learning_rate.R")
```

\section{Transformer performance}
Here we further investigate the performance of the transformer and the effect various perturbations have on its ability to forecast air pollution. As mentioned previously, interactive versions of the figures that visualize the loss during training can be found at \url{https://wandb.ai/mix/mvts-forecasting/reports/Forecasting-Air-Pollution-During-COVID-19--VmlldzoxOTA1Mzc0}.

\subsection{Learning Rate}
\label{sec:lr_comparison}

The effect of the learning rate on model convergence is is examined in Figure \ref{fig:lr_comp}, which shows the training and validation loss for horizons 1 and 5, for each pollutant. It can be observed that, regardless of the horizon, the speed of convergence is faster when the learning rate used for training is 1e-4.  Additionally, the horizon of 5 has in general higher loss during training and takes longer to converge. Finally, note that PM2.5 converges substantially quicker than either NO2 or PM10. Because of this, we only need to view the first 2000k steps of training for PM2.5, whereas visualizing the first 4000k steps for the other pollutants is more informative. We hypothesize that this is due to the lower concentration of PM2.5 in Brussels, making this time series less dynamic than the others. Appendices \ref{appendix:model_comp} and \ref{appendix:lre5} contain results for all horizons and both learning rates.

Tables \ref{tab:mean_df_lr_1e-4} and \ref{tab:mean_df_lr_1e-5} show the mean difference between the VAR and transformer models, for each pollutant and metric. We can see that for all pollutants, the transformer trained with a learning rate of $1e-4$ has a smaller mean difference than the model trained with a $1e-4$ learning rate. It appears that the learning rate of $1e-4$ is better suited for this forecasting task. We take into consideration that for both learning rates, the models were trained for two hours at maximum, due to computational limitations. It may therefore be the case that the when using the learning rate of $1e-5$ the weights take longer to converge and that two hours is not enough time to allow this to happen. Figures such as \ref{fig:no2_loss_train} and \ref{fig:pm25_loss_train} do not support this conclusion however, as it appears that the loss for the $1e-5$ model has converged.


\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_loss_train.png}
  \caption{}
  \label{fig:no2_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_loss_val.png}
  \caption{}
  \label{fig:no2_loss_val}
 \end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_loss_train.png}
  \caption{}
  \label{fig:pm25_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_loss_val.png}
  \caption{}
  \label{fig:pm25_loss_val}
 \end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_loss_train.png}
  \caption{}
  \label{fig:pm10_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_loss_val.png}
  \caption{}
  \label{fig:pm10_loss_val}
 \end{subfigure}

\caption{Training and validation loss when when using learning rates of 1e-4 and 1e-5, for each pollutant}
\label{fig:lr_comp}
\end{figure}


\subsection{Confirmation of Model Behavior}

We now turn our focus onto the transformer model, and observe how its behavior changes in a variety of settings. To simplify the analysis, we focus on a model trained for forecasting at horizon 1, for NO$_2$, with a learning rate of 1e-4. We choose NO$_2$ because that is the pollutant for which the decrease in air pollution during the lockdowns was the most clear and is thus the main pollutant of interest for this study.

We first conduct a simple experiment to confirm that the forecasting utility is working as expected. We first provide the model with no forecasting mask, and train it to predict a horizon of 0, that is. The model receives the labels in the input data, and is also allowed to look into the future. We would suspect that this task is trivial for the model to learn. We then run another experiment in which the model is trained to predict a horizon of 1, but is still allowed to look ahead. Finally, we compare these models to one in which the forecast mask is applied and the model is trained to predict 1 horizon ahead. These experiments are conducted on PM$_{2.5}$, as these models tend to converge the fastest, for horizon 1.

The experiments shown here were conducted to test the effect of including the causal mask in generating forecasts with the multivariate time series transformer. The purpose of the causal mask (which is referred to as forecast mask) is to prevent the model from looking ahead into the future. It masks out the future values for a given current time step in the self-attention mechanism. Horizons of 0 and 1 were compared, with and without the mask.  The logic of these experiments is that the loss should be more easily optimized when the mask is not included, as the model is therefore not allowed use future information. Including the 0 step-ahead forecasts is a sort of sanity check of the model performance, because (1) the model should easily minimize the loss when provided with the labels as input, and (2) this loss should be much lower than the model trained for generating 1 step-ahead forecasts, regardless if the mask is included.

It can be observed that the models trained for 0 step-ahead forecasting converge much quicker than those trained for 1 step-ahead forecasting. This is simply because these models have access the to the target, which is by definition provided in the input. In this case the forecast mask has little effect. However, for 1 step-ahead forecasting, the models trained without the forecast mask reach a lower overall loss (after 1 hour of training time). This is true across all pollutants analyzed: NO2, PM2.5, and PM10. The effect for PM2.5 is smaller because the model more easily minimizes the loss for this pollutant, which was observed to be the case across all experiments.

Note that these figures show only a subset of training steps, where the effect of the mask was most evident in the difference between the losses. In total, these models were trained for approximately 8000 steps, with a learning rate of 1e-4.

\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_forecast_train.png}
  \caption{}
  \label{fig:no2_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_forecast_val.png}
  \caption{}
  \label{fig:no2_forecast_val}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_forecast_train.png}
  \caption{}
  \label{fig:pm25_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_forecast_val.png}
  \caption{}
  \label{fig:pm25_forecast_val}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_forecast_train.png}
  \caption{}
  \label{fig:pm10_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_forecast_val.png}
  \caption{}
  \label{fig:pm10_forecast_val}
\end{subfigure}
\caption{Training and validation loss when including and removing the causal mask, for each pollutant}
\label{fig:mask_removal}
\end{figure}

The model performance evaluated on the test set is shown in Tables \ref{tab:forecast_mask_pm25}, \ref{tab:forecast_mask_pm10}, and \ref{tab:forecast_mask_no2}. We can see that the forecasts for horizon 0 always have the lowest MAE. For forecasts at horizon 1, the error for those generated without the mask always was lower than those generated with the mask. This validates the expected behavior of the model.
```{r forecast-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_forecast.R")
```
\subsection{Variable ablation}
In order to determine which inputs are useful for predicting the pollution levels, we conduct a set of feature ablation experiments to determine the effect of the input variables on the ability of the model to forecast. To do so, we simply train the model with a subset of the original variables. Focusing just on predicting NO$_2$ with a horizon of 1 and a learning rate of 1e-4, we separately remove PM$_{10}$, PM$_{2.5}$, COVID, all tunnels at once, and all variables except lagged NO$_2$.

Figures \ref{fig:ablation_train} and \ref{fig:ablation_val} illustrate the effect that removing the variables has on the transformer's training and validation loss. The run titled "all" indicates that all variables were removed except for the lagged values of NO$_2$. The purpose of this run was to explore the extent to which the model ONLY relies on the univariate input of NO$_2$. 

The primary takeaway is that performance does indeed suffer when the model is used in the univariate setting (run "all"), as can be observed from the relatively higher loss throughout the training/validation procedure. From the forecast mask experiments, there was a relatively small difference in loss between runs with and without the the forecast mask, for a horizon of 1. It was this observation that encouraged us to more closely examine which of the lagged variables contributed to the transformer's performance, particularly in the univariate case. Additionally, we can see in the run "tunnels", ablation of just the tunnels from the input has a rather large impact. In order of performance decrease, "tunnels" is followed by PM$_{25}$, COVID, and PM_${10}$ ( as measured in the last few thousand steps of the validation loss). Of course, there will be a station-specific effect for each of these experiments, which we discuss below.


\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/ablation_train.png}
  \caption{}
  \label{fig:ablation_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/ablation_val.png}
  \caption{}
  \label{fig:ablation_val}
 \end{subfigure}

\caption{Effect on loss of removing variables from input during training}
\label{fig:ablation_loss}
\end{figure}

The model performance for the ablation experiments is shown in Table \ref{tab:ablation_no2}. The value for the removed variable with the highest MAE for a given station is in bold. The leftmost column shows the original model, with all variables included. We can then check to see which variables increased or decreased the MAE. Importantly for the question at the heart of this study, there are many stations for which, when COVID was removed, the MAE increased, though not necessarily by significant amounts. In fact, in stations such as STA-BETN029 or STA-BETR710, the removal of COVID caused the MAE to increase by more than when all variables except lagged NO$_2$. Interestingly, there are many stations for which the MAE also increased substantially when PM10 was removed, which may be due to the similar emission sources of NO$_2$ and PM_${10}$. In no cases did removing the tunnels result in the largest increase in MAE, and only in one case did the removal of PM$_{2.5}$ result in the largest increase (STA-BETN113).
```{r ablation-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_ablation.R")
```
\subsection{Effect of Station Information}

The VAR model has complete access to air measuring station information, that is, it is trained on only one station. In contrast, the transformer is trained on all stations at once, due to the fact that it is impractical to train a separate model for each station. In general, it is important to that the model learns how to generalize between stations. But in the results discussed so far, the model is station-agnostic and has no access to information regarding station it is currently generating forecasts for. Therefore here we investigated the effect of providing station information to the model. We take the rather naive approach of encoding the stations as an ordinal variable, so that each station has a unique integer value. While more sophisticated representations of categorical variable information are certainly available, such as learned embeddings and one-hot representations, we provide this ordinal value directly to the model. It may also be beneficial to include geographical information, but this was not provided in the dataset as collected from the EEA. In these experiments we use a learning rate of 1e-4, and generate forecasts on all horizons (1, 5, and 10 days) for only NO$_2$.

The forecast MAE for each horizon of NO$_2$ are shown in Tables \ref{tab:station_no2_h-1}, \ref{tab:station_no2_h-5}, and \ref{tab:station_no2_h-10}. The tables for the other pollutants can be found in Appendix \ref{appendix:station}. The effect of the station information as an ordinal variable in the case of NO$_2$ does appear to have a somewhat consistent effect of decreasing the MAE, particularly for horizons 5 and 10. However, this effect is not consistent across the pollutants, particularly for PM$_{2.5}$ horizons 1 and 5, where the station variable only seems to hurt forecast performance.
 
We further examine these changes in forecasting error by running the DM test on the difference of the absolute errors for each chemical, station, and horizon. The p-values from the DM tests for NO$_2$ can be seen in Table \ref{tab:dm_station_no2}, and in Tables \ref{tab:dm_station_pm10}, and \ref{tab:dm_station_pm25} in the appendix. It is clear that many of the observed differences are not significant (those that are not bolded at the $p\le.05$ level). For NO$_2$, there are a few stations for which station information did decrease forecasting error by a significant amount, such STA-BETM705 at horizon 5. Therefore, we conclude that either (1) the naive method of encoding station information as an ordinal variable is not effective or (2) station is in fact not an important variable. However, to differentiate these points, more effective ways of providing station in the model would need to be provided. This could be in the form of geographical information, or by creating a separate embedding for each station that is concatenated to the output of the initial projection layer.

```{r station-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_station.R")
```

---
title: "VAR and Transformer Forecast evaluation"
author: "Max Savery"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{caption}
   - \usepackage{subcaption}

output:
  pdf_document:
   keep_tex: TRUE
  html_document: default
---
```{r setup, include=FALSE, results='hide', echo=FALSE}
knitr::is_latex_output()
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stringr)
library(kableExtra)
#  Use the development version from github, so we can use the 
# positive-ensured variance estimator
# install.packages("remotes")
# remotes::install_github("robjhyndman/forecast")
library(forecast)
```
```{r env, results='hide', echo=FALSE}
# Note that the figures generated by wandb are stored in the evaluation directory
# so that they can be generated in the pdf created by this markdown script.
user_dir = "D:/asus_documents/ku_leuven/thesis/code/multi-modal-pollution/src/evaluation"
setwd(user_dir)
```
\section{Model comparison}

We have so far discussed the data used for this work, given a brief theoretical discussion of the models used for forecasting, and described the means by which we can compare their performance. Therefore, we are now prepared to examine the ability of the transformer and the VAR model to forecast air pollution during the lockdowns in Brussels during the COVID-19 pandemic. We compare the models using forecasts generated from the test set, which contains time series of air pollution for the the period of April 24 to June 1st, 2021. The reason for this particular split was mentioned previously. We compare forecasts from the models for each measuring station, for NO$_2$, PM$_{2.5}$, and PM$_{10}$. The RMSE and MAE metrics are shown in Tables \ref{tab:lr-1e-4_pm25_h-1}, \ref{tab:lr-1e-4_pm10_h-1}, and \ref{tab:lr-1e-4_no2_h-1}. These tables show the forecasts from a transformer trained with a learning rate of 1e-4 only, and only 1 step ahead forecasts. Appendix X shows these results for additional horizons of 5 and 10. Furthermore, Appendix X shows these results also using a transformer trained with a learning rate of 1e-5. This motivates our next experiment in which we compare effect of the learning rate on the transformer forecasts. This will be discussed in section \ref{sec:lr_comparison}.

The model performance differs somewhat depending on the pollutant in question. However, across all pollutants for a horizon of 1, we can see that the VAR model has a lower RMSE and MAE. Table \ref{tab:mean_df_lr_1e-4} shows the mean difference between the models for each pollutant metric. where the VAR model outperforms the transformer by approximately 1.5-2 units of RMSE and 1-1.5 units of MAE on average.

To compare the extent to which the forecasts are different, we compute the DM statistic between each series of forecast errors, for each station and pollutant. That is, for the respective series of observed and predicted values using horizon $h$ for a given air measuring station, $y_{1+h}, y_{2+h},...,y_{n}$ and $\hat{y}_{1+h},\hat{y}_{1+h},...,\hat{y}_{n}$, we then compute the DM test on the difference of these two series. These are two-sided tests, so that a significant result indicates that the two forecasts are significantly different than each other. Note that we show only results for comparing difference of the absolute errors (as opposed to the squared errors). The p-values from the DM tests are shown in Tables \ref{tab:dm_tests_lr-1e-4_no2}, \ref{tab:dm_tests_lr-1e-4_pm25}, and \ref{tab:dm_tests_lr-1e-4_pm10}, where $p\le.05$ is shown in bold. As mentioned in section \ref{sec:dm_test}, the DM test holds if the difference is there errors is stationary. Appendix X also shows the p-values generated by the Augmented Dickey Fuller test, which tests if the differences of the errors are in fact stationary. Note that in the interpretation of this test, the null hypothesis tests for non-stationarity. Therefore, rejecting this test is evidence in support of stationarity. The majority of the error differences result in the rejection of the test, so that the hypothesis of stationarity is supported and the assumption of the DM test is satisfied. 
```{r learning-rate-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
# echo=FALSE removes code from pdf; results='hide' removes code output
# Useful resources:
# https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# https://stackoverflow.com/questions/53153537/rmarkdown-setting-the-position-of-kable
# https://bookdown.org/yihui/rmarkdown-cookbook/hide-one.html
source("evaluation_learning_rate.R")
```

\section{Transformer performance}
Here we further investigate the performance of the transformer and the effect various perturbations have on its ability to forecast air pollution. Interactive versions of the figures that visualize the loss during training can be found at \url{https://wandb.ai/mix/mvts-forecasting/reports/Forecasting-Air-Pollution-During-COVID-19--VmlldzoxOTA1Mzc0}

\subsection{Learning Rate}
\label{sec:lr_comparison}

The effect of the learning rate on model convergence is is examined in Figure \ref{fig:lr_comp}, which shows the training and validation loss for horizons 1 and 5, for each pollutant. It can be observed that, regardless of the horizon, the speed of convergence is faster when the learning rate used for training is 1e-4.  Additionally, the horizon of 5 has in general higher loss during training and takes longer to converge. Finally, note that PM2.5 converges substantially quicker than either NO2 or PM10. Because of this, we only need to view the first 2000k steps of training for PM2.5, whereas visualizing the first 4000k steps for the other pollutants is more informative. We hypothesize that this is due to the lower concentration of PM2.5 in Brussels, making this time series less dynamic than the others.

\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_loss_train.png}
  \caption{}
  \label{fig:no2_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_loss_val.png}
  \caption{}
  \label{fig:no2_loss_val}
 \end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_loss_train.png}
  \caption{}
  \label{fig:pm25_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_loss_val.png}
  \caption{}
  \label{fig:pm25_loss_val}
 \end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_loss_train.png}
  \caption{}
  \label{fig:pm10_loss_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_loss_val.png}
  \caption{}
  \label{fig:pm10_loss_val}
 \end{subfigure}

\caption{Training and validation loss when when using learning rates of 1e-4 and 1e-5, for each pollutant}
\label{fig:lr_comp}
\end{figure}


\subsection{Confirmation of Model Behavior}

We now turn our focus onto solely the transformer model, and observe how its behavior changes in a variety of setting. To simplify the analysis, we focus on a model trained for forecasting at horizon$=1$, for NO$_2$, with a learning rate of 1e-4. We choose this model because... well because NO$_2$ is the most important.

We first conduct a simple experiment to confirm that the forecasting utility is working as conducted. We first provide the model with no forecasting mask, and train it to predict a horizon of 0, that is. The model receives the labels in the input data, and is also allowed to look into the future. We would suspect that this task is trivial for the model to learn. We then run another experiment in which the model is trained to predict a horizon of 1, but is still allowed to look ahead. Finally, we compare these models to one in which the forecast mask is applied and the model is trained to predict 1 horizon ahead. These experiments are conducted on PM$_{2.5}$, as these models tend to converge the fastest, for horizon 1.

The experiments shown here were conducted to test the effect of including the causal mask in generating forecasts with the multivariate time series transformer. The purpose of the causal mask (which is referred to as forecast mask) is to prevent the model from looking ahead into the future. It masks out the future values for a given current time step in the self-attention mechanism. Horizons of 0 and 1 were compared, with and without the mask.  The logic of these experiments is that the loss should be more easily optimized when the mask is not included, as the model is therefore not allowed use future information. Including the 0 step-ahead forecasts is a sort of sanity check of the model performance, because (1) the model should easily minimize the loss when provided with the labels as input, and (2) this loss should be much lower than the model trained for generating 1 step-ahead forecasts, regardless if the mask is included.

It can be observed that the models trained for 0 step-ahead forecasting converge much quicker than those trained for 1 step-ahead forecasting. This is simply because these models have access the to the target, which is by definition provided in the input. In this case the forecast mask has little effect. However, for 1 step-ahead forecasting, the models trained without the forecast mask reach a lower overall loss (after 1 hour of training time). This is true across all pollutants analyzed: NO2, PM2.5, and PM10. The effect for PM2.5 is smaller because the model more easily minimizes the loss for this pollutant, which was observed to be the case across all experiments.

These figures shown only a subset of training steps, where the effect of the mask was most evident in the difference between the losses. In total, these models were trained for approximately 8000 steps, with a learning rate of 1e-4.

\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_forecast_train.png}
  \caption{}
  \label{fig:no2_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/no2_forecast_val.png}
  \caption{}
  \label{fig:no2_forecast_val}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_forecast_train.png}
  \caption{}
  \label{fig:pm25_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm25_forecast_val.png}
  \caption{}
  \label{fig:pm25_forecast_val}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_forecast_train.png}
  \caption{}
  \label{fig:pm10_forecast_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/pm10_forecast_val.png}
  \caption{}
  \label{fig:pm10_forecast_val}
\end{subfigure}
\caption{Training and validation loss when including and removing the causal mask, for each pollutant}
\label{fig:mask_removal}
\end{figure}

The model performance evaluated on the test set is shown in Tables \ref{tab:forecast_mask_pm25}, \ref{tab:forecast_mask_pm10}, and \ref{tab:forecast_mask_no2}. We can see that the forecasts for horizon 0 always have the lowest MAE. For forecasts at horizon 1, the error for those generated without the mask always was lower than those generated with the mask. This validates the expected behavior of the model.
```{r forecast-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_forecast.R")
```
\subsection{Variable ablation}
In order to determine which inputs are useful for predicting the pollution levels, we conduct a set of feature ablation experiments to determine the effect of the input variables on the ability of the model to forecast. In the context of transformers, the attention mechanism is not necessarily interpretable (SOURCE), and why there is research examining ways in which to make the interpretable, this is a developing area and no standard method has been developed. Therefore, the most accessible means to examine feature importance is via ablation from the model. Here we simply train the model with a subset of the original variables. We leave in the traffic measuring stations, as they are not explicitly relevant for our scientific question. Focusing just on predicting NO$_2$, we remove PM$_{10}$, PM$_{2.5}$, and COVID

From these results it can be seen that the model trained without COVID does take longer to converge and that performance does decrease when COVID is not included as a variable. However, the change is marginal, indicating that the most important information is coming from the past values. This observation is also supported by the previous experiments, in which the difference between performance between the model with and without the forecast mask is not large.

The model performance for horizon 1 are shown in Table \ref{tab:ablation_no2}. The variable, once removed, with the highest MAE for a given station is in bold. The leftmost column shows the original model, with all variables included. We can then check to see which variables increased or decreased the MAE. Importantly for the question at the heart of this study, there are many stations for which, when the COVID variable was removed, the MAE increased by over 1 unit of MAE. Interestingly, there are many stations for which the MAE also increased when PM10 was removed.
```{r ablation-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_ablation.R")
```

Figures \ref{fig:ablation_train} and \ref{fig:ablation_val} illustrate the effect that removing variables from the input to the transformer has on the training and validation loss. In the previous experiments, 9 variables were used as input:  NO2, PM2.5, PM10, COVID-19 frequencies in Flanders, Traffic volume for 5 different traffic measuring stations. These variables were lagged h steps behind the target output, depending on the horizon desired for forecasting. For example, for horizon=1, we would only use the variables from the previous day and earlier. But for horizon=5, we use variables 5 days behind and earlier.  But in the experiment here, we remove each variable to see the effect on performance.
For simplicity, the runs were conducted with a training loss of 1e-4 and only for NO2, and with a forecasting horizon of 1. The run titled "all" indicates that all variables were removed except for the lagged values of NO2. The purpose of this run was to explore the extent to which the model ONLY relies on the univariate input of NO2. 
The primary takeaway is that performance does indeed suffer when the model is used in the univariate setting. From earlier experiments, this was not necessarily clear. Additionally, we can see in the run "tunnels" that ablation of just the tunnels from the input has a rather large impact, followed by PM25, COVID, and PM10 (in the last few thousand steps of the validation loss). Of course, there will be a station-specific effect for each of these experiments, and these results will be discussed in the thesis itself where models forecast values from the test set.


\begin{figure}[H]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/ablation_train.png}
  \caption{}
  \label{fig:ablation_train}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\linewidth]{Figures/training/ablation_val.png}
  \caption{}
  \label{fig:ablation_val}
 \end{subfigure}

\caption{Effect on loss of removing variables from input during training}
\label{fig:ablation_loss}
\end{figure}

\subsection{Effect of Station Information}

The VAR model has complete access to air measuring station information, that is, it is trained on only one station. In contrast, the transformer is trained on all stations at once, due to the fact that it is impractical to train a separate model for each station and so that the model can learn how to generalize between stations. But in the results discussed so far, the model is station-agnostic and has no access to information on which station it is currently generating forecasts for. We were therefore interested to see the effect of providing station information to the model. We take the rather naive approach of encoding the stations as an ordinal variable, so that each station has a unique integer value. While more sophisticated representations of categorical variable information are certainly available, such as learned embeddings and one-hot representations, we provide this ordinal value directly to the model. It may also be beneficial to include geographical information, but this was not provided in the dataset as collected from the EEA. In these experiments we use a batch size of 16, a learning rate of 1e-4, and generate forecasts on all horizons (1, 5, and 10 days).

The tables for the first horizon for each chemical are shown in Tables \ref{tab:station_no2_h-1}, \ref{tab:station_no2_h-5}, and \ref{tab:station_no2_h-10}. The tables for the other horizons can be found in Appendix \ref{appendix:station}. The effect of the station information as an ordinal variable is not clear. There may be some pollutant-specific or horizon-specific contribution; for example, it appears that RMSE and MAE are frequently lower for NO$_2$, horizon 5 and 10, when the station variable is included. However, this effect is not consistent across the pollutants.
 
We further examine these changes in forecasting error by running the DM test on the difference of the absolute errors for each chemical, station, and horizon. The p-values from the DM tests for NO$_2$ can be seen in Table \ref{tab:dm_station_no2}, and in Tables \ref{tab:dm_station_pm10}, and \ref{tab:dm_station_pm25} in the appendix for PM$_{10}$ and PM$_{2.5}$. It is clear that many of the observed differences are not significant (those that are not bolded at the $p\le.05$ level). For NO$_2$, there are a few stations for which station did decrease forecasting error by a significant amount, particularly for the forecasts of horizon 10. This is true in the case of stations STA-BETN093 and STA-BETN100, for example. But in general, these are the exceptions. Therefore, we conclude that either (1) the naive method of encoding station information as an ordinal variable is not effective or (2) station is in fact not an important variable. However, to determine the second point, more effective ways of providing station in the model would need to be provided. This could be in the form of geographical information, or by creating a separate embedding for each station that is concatenated to the output of the initial projection layer.

```{r station-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_station.R")
```

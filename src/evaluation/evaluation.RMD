---
title: "VAR and Transformer Forecast evaluation"
author: "Max Savery"
header-includes:
   - \usepackage{amsmath}
output:
  pdf_document:
   keep_tex: TRUE
  html_document: default
---
```{r setup, include=FALSE, results='hide', echo=FALSE}
knitr::is_latex_output()
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stringr)
library(kableExtra)
#  Use the development version from github, so we can use the 
# positive-ensured variance estimator
# install.packages("remotes")
# remotes::install_github("robjhyndman/forecast")
library(forecast)
```
```{r env, results='hide', echo=FALSE}
user_dir = "D:/asus_documents/ku_leuven/thesis/code/multi-modal-pollution/src/evaluation"
setwd(user_dir)
```
\section{Model comparison}

We compare models the models on the period of April 24 to June 1st, 2021. As mentioned previously, this time period was used as our test set. We compare forecast horizons of 1, 5, and 10 steps for each measuring per station, per model. The model evaluations are shown for each horizon separately, as can be seen in Tables X, Y, and Z, each of which show the RMSE and MAE for each model. For each evaluation, the test dataset is normalized by the training data mean and standard deviation.

Note that the DM tests are done on MAE alone.

To compare the differences between the forecasts errors, we compute the DM statistic between each series of forecast errors. That is, given a series observed and predicted values for a station and a horizon, $y^h_{1},...,y^h_{n}$ and $\hat{y}^h_{1},...,\hat{y}^h_{n}$, we then compute the DM test on the difference of these two series. This gives us, for each chemical and horizon, 41 tests, one for each station. The p-values of these tests are shown in tables X, Y, and Z, one for each chemical. We also show the P-values generated by the Augmented Dickey Fuller test, which tests if the differences of the errors are in factor stationary. Note, however, that the null hypothesis tests for non-stationarity. Therefore, rejecting this test is evidence in support of stationarity. We can see in tables A,B, and C that for the majority of the error differences, the hypothesis of stationarity is supported, satisfying the assumptiuon of the DM test.  

 In the computation of the DM statistic, we observed negative variance estimations. This is discussed here \url{https://www.sciencedirect.com/science/article/pii/S0169207017300559}. We therefore use the Bartlett estimator.

The comparison of each model trained with a different learning rate is shown in Tables \ref{tab:lr-1e-4_no2_h-1}... It can be observed that, regardless of the horizon, the speed of convergence is faster when the learning rate used for training is 1e-4.  Additionally, the horizon of 5 has in general higher loss during training and takes longer to converge. Finally, note that PM2.5 converges substantially quicker than either NO2 or PM10. Because of this, we only need to view the first 2000k steps of training for PM2.5, whereas visualizing the first 4000k steps for the other pollutants is more informative. We hypothesize that this is due to the lower concentration of PM2.5 in Brussels, making this time series less dynamic than the others.

The DM tests comparing the models are shown in Tables \ref{tab:dm_tests_lr-1e-4_no2}...
```{r learning-rate-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
# echo=FALSE removes code from pdf; results='hide' removes code output
# Useful resources:
# https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# https://stackoverflow.com/questions/53153537/rmarkdown-setting-the-position-of-kable
# https://bookdown.org/yihui/rmarkdown-cookbook/hide-one.html
source("evaluation_learning_rate.R")
```
\section{Confirmation of Model Behavior}

We now turn our focus onto solely the transformer model, and observe how its behavior changes in a variety of setting. To simplify the analysis, we focus on a model trained for forecasting at horizon$=1$, for NO$_2$, with a learning rate of 1e-4. We choose this model because... well because NO$_2$ is the most important. 

Interactive versions of the figures that visualize the loss during training can be found at \url{https://wandb.ai/mix/mvts-forecasting/reports/Forecasting-Air-Pollution-During-COVID-19--VmlldzoxOTA1Mzc0}

First, we conduct a simple performance validation, to observe the ability of the model to learn the data. We first compare learning rates of $1e-3$, $1e-4$, and $1e-5$. It can be observed that the smaller learning rate does make the learning process slower, but offers significant performance gains in terms of minimum average loss achieved.

We also conduct a simple experiment to confirm that the forecasting utility is working as conducted. We first provide the model with no forecasting mask, and train it to predict a horizon of 0, that is. The model receives the labels in the input data, and is also allowed to look into the future. We would suspect that this task is trivial for the model to learn. We then run another experiment in which the model is trained to predict a horizon of 1, but is still allowed to look ahead. Finally, we compare these models to one in which the forecast mask is applied and the model is trained to predict 1 horizon ahead. These experiments are conducted on PM$_{2.5}$, as these models tend to converge the fastest, for horizon 1.

The model performance is also shown in Tables \ref{tab:forecast_mask_pm25}, \ref{tab:forecast_mask_pm10}, and \ref{tab:forecast_mask_no2}.
```{r forecast-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_forecast.R")
```
\section{Variable ablation}
In order to determine which inputs are useful for predicting the pollution levels, we conduct a set of feature ablation experiments to determine the effect of the input variables on the ability of the model to forecast. In the context of transformers, the attention mechanism is not necessarily interpretable (SOURCE), and why there is research examining ways in which to make the interpretable, this is a developing area and no standard method has been developed. Therefore, the most accessible means to examine feature importance is via ablation from the model. Here we simply train the model with a subset of the original variables. We leave in the traffic measuring stations, as they are not explicitly relevant for our scientific question. Focusing just on predicting NO$_2$, we remove PM$_{10}$, PM$_{2.5}$, and COVID

From these results it can be seen that the model trained without COVID does take longer to converge and that performance does decrease when COVID is not included as a variable. However, the change is marginal, indicating that the most important information is coming from the past values. This observation is also supported by the previous experiments, in which the difference between performance between the model with and without the forecast mask is not large.

The model performance for horizon 1 are shown in Table \ref{tab:ablation_no2}. The variable, once removed, with the highest MAE for a given station is in bold. The leftmost column shows the original model, with all variables included. We can then check to see which variables increased or decreased the MAE. Importantly for the question at the heart of this study, there are many stations for which, when the COVID variable was removed, the MAE increased by over 1 unit of MAE. Interestingly, there are many stations for which the MAE also increased when PM10 was removed. 

 TODO: Look at units of NO2 and what this means. For now I will leave it

```{r ablation-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_ablation.R")
```
\section{Effect of Station Information}
Because the arima model has complete access to station information, that is, it is trained on only one station, we were curious to see the effect of including station information to the model. Therefore, we examine the effect of the ordinal station variable. While more sophisticated representations of categorical variable information are certainly available, such as learned embeddings and one-hot representations, we chose to simply encode the stations as an ordinal variable, and provide it directly to the model. It is quite possible that the transformer would be able to effectively utilize this information to adjust its predictions per station. The justification for including this feature is that the VAR model is trained per measuring station, whereas the transformer is trained on all stations at once. Therefore, providing information to the model would theoretically be quite useful. Other information we could provide would include geographical information, but this was not provided in the dataset gathered from the EEA.

We use a batch size of 16, a learning rate of 1e-4 for these experiments, but run the models on all horizons (1, 5, and 10 days). 

The tables for the first horizon for each chemical are shown in Tables \ref{tab:station_no2_h-1}, \ref{tab:station_pm10_h-1}, and \ref{tab:station_pm25_h-1}. The tables for the other horizons can be found in the Appendix. The effect of the station information as an ordinal variable is not clear. There may be some pollutant-specific or horizon-specific contribution; for example, it appears that RMSE and MAE are frequently lower for PM$_{10}$, horizon 1 and 5, when the station variable is included. However, this effect is not consistent 
 
We further examine these changes in forecasting error by running the DM test for each chemical, station, and horizon. The p-values from the DM tests can be seen in Figures \ref{tab:dm_station_no2}, \ref{tab:dm_station_pm10}, and \ref{tab:dm_station_pm25}. It is clear that many of the observed differences are not significant. Therefore, we conclude that (1) the naive method of encoding station information as an ordinal variable is not effective or (2) station is truly not an important variable. However, to determine the second point, more effective ways of providing station in the model would need to be provided. This could be in the form of geographical information, or by creating a separate embedding for each station that is added to the output of the initial projection layer.

```{r station-evaluation, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
source("evaluation_station.R")
```
